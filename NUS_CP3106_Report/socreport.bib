
@inproceedings{pontiki-etal-2014-semeval,
    title = "{S}em{E}val-2014 Task 4: Aspect Based Sentiment Analysis",
    author = "Pontiki, Maria  and
      Galanis, Dimitris  and
      Pavlopoulos, John  and
      Papageorgiou, Harris  and
      Androutsopoulos, Ion  and
      Manandhar, Suresh",
    booktitle = "Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S14-2004",
    doi = "10.3115/v1/S14-2004",
    pages = "27--35",
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/Users/hover/Zotero/storage/865KQ9AE/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/hover/Zotero/storage/UNER9X29/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language,SOURCE},
  primaryClass = {cs}
}



@inproceedings{zhang-etal-2019-ernie,
    title = "{ERNIE}: Enhanced Language Representation with Informative Entities",
    author = "Zhang, Zhengyan  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Jiang, Xin  and
      Sun, Maosong  and
      Liu, Qun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1139",
    doi = "10.18653/v1/P19-1139",
    pages = "1441--1451",
    abstract = "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
}



@article{jiSurveyKnowledgeGraphs2020a,
  title = {A {{Survey}} on {{Knowledge Graphs}}: {{Representation}}, {{Acquisition}} and {{Applications}}},
  shorttitle = {A {{Survey}} on {{Knowledge Graphs}}},
  author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
  year = {2020},
  month = feb,
  abstract = {Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction towards cognition and human-level intelligence. In this survey, we provide a comprehensive review on knowledge graph covering overall research topics about 1) knowledge graph representation learning, 2) knowledge acquisition and completion, 3) temporal knowledge graph, and 4) knowledge-aware applications, and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference and logical rule reasoning are reviewed. We further explore several emerging topics including meta relational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of datasets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.},
  archivePrefix = {arXiv},
  eprint = {2002.00388},
  eprinttype = {arxiv},
  file = {/Users/hover/Zotero/storage/QXEXANVC/Ji et al. - 2020 - A Survey on Knowledge Graphs Representation, Acqu.pdf},
  journal = {arXiv:2002.00388 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{cambria2018senticnet,
	Author = {Cambria, Erik and Poria, Soujanya and Hazarika, Devamanyu and Kwok, Kenneth},
	Booktitle = {AAAI},
	Title = {{SenticNet} 5: Discovering conceptual primitives for sentiment analysis by means of context embeddings},
	Year = {2018},
	pages ={1795--1802}
}

@inproceedings{ma2018targeted,
	Author = {Ma, Yukun and Peng, Haiyun and Cambria, Erik},
	Booktitle = {AAAI},
	Title = {Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM},
	pages={5876--5883},
	Year = {2018}
}



@inproceedings{Tang2016,
abstract = {We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.},
archivePrefix = {arXiv},
arxivId = {1605.08900v2},
author = {Tang, Duyu and Qin, Bing and Liu, Ting},
booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D16-1021},
eprint = {1605.08900v2},
file = {:home/alexander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Qin, Liu - 2016 - Aspect Level Sentiment Classification with Deep Memory Network.pdf:pdf},
pages = {214--224},
title = {{Aspect Level Sentiment Classification with Deep Memory Network}},
url = {https://arxiv.org/pdf/1605.08900v2.pdf http://aclweb.org/anthology/D16-1021},
year = {2016}
}


@inproceedings{Tang2015,
abstract = {Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.},
archivePrefix = {arXiv},
arxivId = {1512.01100},
author = {Tang, Duyu and Qin, Bing and Feng, Xiaocheng and Liu, Ting},
booktitle = {COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers},
eprint = {1512.01100},
file = {:home/alexander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2015 - Effective LSTMs for Target-Dependent Sentiment Classification.pdf:pdf},
isbn = {9784879747020},
pages = {3298--3307},
title = {{Effective LSTMs for target-dependent sentiment classification}},
url = {http://ir.hit.edu.cn/ http://arxiv.org/abs/1512.01100},
year = {2016}
}

@article{ArxSong,
  title={Attentional encoder network for targeted sentiment classification},
  author={Song, Youwei and Wang, Jiahai and Jiang, Tao and Liu, Zhiyue and Rao, Yanghui},
  journal={arXiv preprint arXiv:1902.09314},
  url       = {http://arxiv.org/abs/1902.09314},
  year={2019}
}

@article{ArxZhaoa2019,
  title={Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect-level Sentiment Classification},
  author={Zhaoa, Pinlong and Houb, Linlin and Wua, Ou},
  journal={arXiv preprint arXiv:1906.04501},
  url={http://arxiv.org/abs/1906.04501},
  year={2019}
}

@inproceedings{Li,
abstract = {Aspect-level sentiment classification (ASC) aims at identifying sentiment polarities towards aspects in a sentence, where the aspect can behave as a general Aspect Category (AC) or a specific Aspect Term (AT). However, due to the especially expensive and labor-intensive labeling, existing public corpora in AT-level are all relatively small. Meanwhile, most of the previous methods rely on complicated structures with given scarce data, which largely limits the efficacy of the neural models. In this paper, we exploit a new direction named coarse-to-fine task transfer, which aims to leverage knowledge learned from a rich-resource source domain of the coarse-grained AC task, which is more easily accessible, to improve the learning in a low-resource target domain of the fine-grained AT task. To resolve both the aspect granularity inconsistency and feature mismatch between domains, we propose a Multi-Granularity Alignment Network (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary task can help the AC task modeling at the same fine-grained level with the AT task. To alleviate the feature false alignment, a contrastive feature alignment method is adopted to align aspect-specific feature representations semantically. In addition, a large-scale multi-domain dataset for the AC task is provided. Empirically, extensive experiments demonstrate the effectiveness of the MGAN.},
archivePrefix = {arXiv},
arxivId = {1811.10999},
author = {Li, Zheng and Wei, Ying and Zhang, Yu and Zhang, Xiang and Li, Xin and Yang, Qiang},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
eprint = {1811.10999},
file = {:home/alexander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification.pdf:pdf},
pages = {4253----4260},
title = {{Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification}},
url = {www.aaai.org http://arxiv.org/abs/1811.10999},
year = {2019}
}

@inproceedings{He,
abstract = {Attention-based long short-term memory (LSTM) networks have proven to be useful in aspect-level sentiment classification. However, due to the difficulties in annotating aspect-level data, existing public datasets for this task are all relatively small, which largely limits the effectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document- level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classification. We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014, 2015, and 2016, and we show that attention-based LSTM benefits from document-level knowledge in multiple ways.},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
booktitle = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
doi = {10.18653/v1/p18-2092},
file = {:home/alexander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2019 - Exploiting Document Knowledge for Aspect-level Sentiment Classification.pdf:pdf},
isbn = {9781948087346},
keywords = {Transfer},
mendeley-tags = {Transfer},
pages = {579--585},
title = {{Exploiting document knowledge for aspect-level sentiment classification}},
url = {https://github.com/},
volume = {2},
year = {2018}
}


@inproceedings{He2016,
address = {New York, New York, USA},
author = {He, Ruining and McAuley, Julian},
booktitle = {Proceedings of the 25th International Conference on World Wide Web - WWW '16},
doi = {10.1145/2872427.2883037},
isbn = {9781450341431},
keywords = {fashion evolution,personalized ranking,recommender systems,visual dimensions},
pages = {507--517},
publisher = {ACM Press},
title = {{Ups and Downs}},
url = {http://dl.acm.org/citation.cfm?doid=2872427.2883037},
year = {2016}
}

@article{Hochreiter1997,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{Xu2019,
abstract = {Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions.{\~{}}We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective. The datasets and code are available at https://www.cs.uic.edu/{\~{}}hxu/.},
archivePrefix = {arXiv},
arxivId = {1904.02232},
author = {Xu, Hu and Liu, Bing and Shu, Lei and Yu, Philip S},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
eprint = {1904.02232},
file = {:home/alexander/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2019 - BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis.pdf:pdf},
pages = {2324--2335},
publisher = {Association for Computational Linguistics},
title = {{BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis}},
url = {https://www. http://arxiv.org/abs/1904.02232},
year = {2019}
}


@article{Song2019,
   title={Targeted Sentiment Classification with Attentional Encoder Network},
   ISBN={9783030304904},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-030-30490-4_9},
   DOI={10.1007/978-3-030-30490-4_9},
   journal={Lecture Notes in Computer Science},
   publisher={Springer International Publishing},
   author={Song, Youwei and Wang, Jiahai and Jiang, Tao and Liu, Zhiyue and Rao, Yanghui},
   year={2019},
   pages={93–103}
}

@misc{pei2019targeted,
    title={Targeted Sentiment Analysis: A Data-Driven Categorization},
    author={Jiaxin Pei and Aixin Sun and Chenliang Li},
    year={2019},
    eprint={1905.03423},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{karimi2020adversarial,
    title={Adversarial Training for Aspect-Based Sentiment Analysis with BERT},
    author={Akbar Karimi and Leonardo Rossi and Andrea Prati and Katharina Full},
    year={2020},
    eprint={2001.11316},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{miyato2016adversarial,
    title={Adversarial Training Methods for Semi-Supervised Text Classification},
    author={Takeru Miyato and Andrew M. Dai and Ian Goodfellow},
    year={2016},
    eprint={1605.07725},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{xu2019bert,
    title={BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis},
    author={Hu Xu and Bing Liu and Lei Shu and Philip S. Yu},
    year={2019},
    eprint={1904.02232},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@ARTICLE{8864964,  author={Z. {Gao} and A. {Feng} and X. {Song} and X. {Wu}},  journal={IEEE Access},  title={Target-Dependent Sentiment Classification With BERT},   year={2019},  volume={7},  number={},  pages={154290-154299},}



@inproceedings{mitchell2013open,
  title={Open domain targeted sentiment},
  author={Mitchell, Margaret and Aguilar, Jacqui and Wilson, Theresa and Van Durme, Benjamin},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1643--1654},
  year={2013}
}


@misc{soares2019matching,
    title={Matching the Blanks: Distributional Similarity for Relation Learning},
    author={Livio Baldini Soares and Nicholas FitzGerald and Jeffrey Ling and Tom Kwiatkowski},
    year={2019},
    eprint={1906.03158},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{sun2019finetune,
    title={How to Fine-Tune BERT for Text Classification?},
    author={Chi Sun and Xipeng Qiu and Yige Xu and Xuanjing Huang},
    year={2019},
    eprint={1905.05583},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{alex2019adapt,
    title={Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification},
    author={Alexander Rietzler and Sebastian Stabinger and Paul Opitz and Stefan Engl},
    year={2019},
    eprint={1908.11860},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}